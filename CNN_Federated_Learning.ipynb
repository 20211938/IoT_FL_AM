{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67c678e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU: NVIDIA GeForce RTX 3070\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print('GPU not found.') if not torch.cuda.is_available() else print(f'Found GPU: {torch.cuda.get_device_name(0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36f09171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: 라이브러리 import\n",
    "import torch\n",
    "from utils.cnn.classifier import initialize_cnn_classifier\n",
    "from utils.cnn.dataset_functions import create_cnn_dataset, unwrap_client_data, get_label_mapping\n",
    "from utils.cnn.federated_averaging import federated_averaging\n",
    "from utils.cnn.visualization import plot_training_curves, visualize_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea477a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client1: 102개 파일 (테스트로 18개 추출)\n",
      "client2: 102개 파일 (테스트로 18개 추출)\n",
      "client3: 102개 파일 (테스트로 18개 추출)\n",
      "client4: 102개 파일 (테스트로 18개 추출)\n",
      "client5: 102개 파일 (테스트로 18개 추출)\n",
      "\n",
      "client6 (테스트): 90개 파일 (다양한 클라이언트에서 추출)\n",
      "총 학습 데이터: 510개\n",
      "총 테스트 데이터: 90개\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: 데이터 경로 및 클라이언트 분배 설정 (수정 버전)\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "data_dir = 'data_train/'\n",
    "imagePath0 = f'{data_dir}/0/'\n",
    "imagePath1 = f'{data_dir}/1/'\n",
    "npyPath = f'{data_dir}/annotations/'\n",
    "\n",
    "# 파일 분배 설정\n",
    "total_files = 600\n",
    "num_clients = 6\n",
    "test_client_id = 6  # 테스트용 클라이언트 ID\n",
    "test_data_ratio = 0.15  # 각 클라이언트에서 추출할 비율 (10%)\n",
    "\n",
    "# 전체 파일 리스트 생성\n",
    "all_files = [f'{i:06d}' for i in range(1, total_files + 1)]\n",
    "random.shuffle(all_files)  # 랜덤 셔플 (선택사항)\n",
    "\n",
    "# client6을 제외한 클라이언트에 데이터 분배\n",
    "train_clients = [f'client{i}' for i in range(1, num_clients + 1) if i != test_client_id]\n",
    "num_train_clients = len(train_clients)\n",
    "files_per_train_client = len(all_files) // num_train_clients\n",
    "\n",
    "clientIdentifierDict = {}\n",
    "test_files = []  # client6에 들어갈 파일들\n",
    "\n",
    "# 각 학습 클라이언트에 데이터 분배 및 테스트 데이터 추출\n",
    "start_idx = 0\n",
    "for i, client_id in enumerate(train_clients):\n",
    "    if i < num_train_clients - 1:\n",
    "        end_idx = start_idx + files_per_train_client\n",
    "    else:\n",
    "        end_idx = len(all_files)\n",
    "    \n",
    "    # 클라이언트에 할당된 파일들\n",
    "    client_files = all_files[start_idx:end_idx]\n",
    "    \n",
    "    # 테스트 데이터 추출 (각 클라이언트에서 일정 비율)\n",
    "    num_test_from_client = int(len(client_files) * test_data_ratio)\n",
    "    test_files_from_client = random.sample(client_files, num_test_from_client)\n",
    "    test_files.extend(test_files_from_client)\n",
    "    \n",
    "    # 남은 파일들을 클라이언트에 할당\n",
    "    train_files = [f for f in client_files if f not in test_files_from_client]\n",
    "    clientIdentifierDict[client_id] = train_files\n",
    "    \n",
    "    print(f'{client_id}: {len(train_files)}개 파일 (테스트로 {num_test_from_client}개 추출)')\n",
    "    start_idx = end_idx\n",
    "\n",
    "# client6에 테스트 파일 할당\n",
    "clientIdentifierDict[f'client{test_client_id}'] = test_files\n",
    "print(f'\\nclient{test_client_id} (테스트): {len(test_files)}개 파일 (다양한 클라이언트에서 추출)')\n",
    "print(f'총 학습 데이터: {sum(len(files) for k, files in clientIdentifierDict.items() if k != f\"client{test_client_id}\")}개')\n",
    "print(f'총 테스트 데이터: {len(test_files)}개')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "502403f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "발견된 결함 유형 (0과 1 제외): [np.int32(-1), np.uint8(2), np.uint8(3), np.uint8(4), np.uint8(5), np.uint8(6), np.uint8(8), np.uint8(255)]\n",
      "레이블 매핑: {np.int32(-1): 0, np.uint8(2): 1, np.uint8(3): 2, np.uint8(4): 3, np.uint8(5): 4, np.uint8(6): 5, np.uint8(8): 6, np.uint8(255): 7}\n",
      "총 클래스 수: 8\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: 레이블 매핑 생성\n",
    "label_mapping, num_classes = get_label_mapping(data_dir, clientIdentifierDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b74657f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 결함 유형 확인\n",
    "# from utils.cnn.visual_npy import visualize_defect_types_samples\n",
    "\n",
    "# visualize_defect_types_samples(data_dir, clientIdentifierDict, label_mapping, min_pixels=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b40b64b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "데이터셋 생성 중...\n",
      "client1...\n",
      "  처리 중: 50/102 파일 완료\n",
      "  처리 중: 100/102 파일 완료\n",
      "Contains 102 images...\n",
      "Valid images: 102 (filtered 0 invalid labels)\n",
      "Image Tensor Shape: torch.Size([102, 2, 640, 640])\n",
      "client2...\n",
      "  처리 중: 50/102 파일 완료\n",
      "  처리 중: 100/102 파일 완료\n",
      "Contains 102 images...\n",
      "Valid images: 102 (filtered 0 invalid labels)\n",
      "Image Tensor Shape: torch.Size([102, 2, 640, 640])\n",
      "client3...\n",
      "  처리 중: 50/102 파일 완료\n",
      "  처리 중: 100/102 파일 완료\n",
      "Contains 102 images...\n",
      "Valid images: 102 (filtered 0 invalid labels)\n",
      "Image Tensor Shape: torch.Size([102, 2, 640, 640])\n",
      "client4...\n",
      "  처리 중: 50/102 파일 완료\n",
      "  처리 중: 100/102 파일 완료\n",
      "Contains 102 images...\n",
      "Valid images: 102 (filtered 0 invalid labels)\n",
      "Image Tensor Shape: torch.Size([102, 2, 640, 640])\n",
      "client5...\n",
      "  처리 중: 50/102 파일 완료\n",
      "  처리 중: 100/102 파일 완료\n",
      "Contains 102 images...\n",
      "Valid images: 102 (filtered 0 invalid labels)\n",
      "Image Tensor Shape: torch.Size([102, 2, 640, 640])\n",
      "client6...\n",
      "  처리 중: 50/90 파일 완료\n",
      "Contains 90 images...\n",
      "Valid images: 90 (filtered 0 invalid labels)\n",
      "Image Tensor Shape: torch.Size([90, 2, 640, 640])\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: 데이터셋 생성\n",
    "target_size = (640, 640)\n",
    "print(\"\\n데이터셋 생성 중...\")\n",
    "imageDict, labelDict = create_cnn_dataset(\n",
    "    clientIdentifierDict,\n",
    "    data_dir,\n",
    "    target_size=target_size,\n",
    "    label_mapping=label_mapping\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3e6b424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Train/Test 분할\n",
    "trainClients = ['client1', 'client2', 'client3', 'client4', \n",
    "                'client5']\n",
    "testClients = ['client6']\n",
    "\n",
    "clientIDs = trainClients\n",
    "\n",
    "# Train Data\n",
    "trainImageDict = {clientID: imageDict[clientID] for clientID in trainClients}\n",
    "trainLabelDict = {clientID: labelDict[clientID] for clientID in trainClients}\n",
    "\n",
    "# Test Data\n",
    "testImageDict = {clientID: imageDict[clientID] for clientID in testClients}\n",
    "testLabelDict = {clientID: labelDict[clientID] for clientID in testClients}\n",
    "\n",
    "testImages, testLabels = unwrap_client_data(testImageDict, testLabelDict, testClients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51a9cc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 레이블 검증\n",
    "# print(\"\\n레이블 범위 검증:\")\n",
    "# for clientID in clientIDs:\n",
    "#     labels = trainLabelDict[clientID]\n",
    "#     min_label = labels.min().item()\n",
    "#     max_label = labels.max().item()\n",
    "#     invalid_count = ((labels < 0) | (labels >= num_classes)).sum().item()\n",
    "#     print(f\"{clientID}: min={min_label}, max={max_label}, invalid={invalid_count}\")\n",
    "#     if invalid_count > 0:\n",
    "#         print(f\"  경고: {clientID}에 유효하지 않은 레이블이 {invalid_count}개 있습니다!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "132f8a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: 하이퍼파라미터 설정\n",
    "SERVER_ROUNDS = 2\n",
    "LOCAL_EPOCHS = 6\n",
    "LOCAL_BATCH_SIZE = 32\n",
    "LOCAL_LEARNING_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e392416c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 초기화 완료! 클래스 수: 8\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: 모델 초기화\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = initialize_cnn_classifier(num_classes, input_channels=2, device=device)\n",
    "print(f\"모델 초기화 완료! 클래스 수: {num_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "597c7050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "------ Server Round 0 ------\n",
      "============================================================\n",
      "\n",
      "Running local updates for client1...\n",
      "Epoch 1/6 - Loss: 2.0888, Accuracy: 0.00%\n",
      "Epoch 2/6 - Loss: 2.0753, Accuracy: 0.98%\n",
      "Epoch 3/6 - Loss: 2.0564, Accuracy: 5.88%\n",
      "Epoch 4/6 - Loss: 2.0106, Accuracy: 29.41%\n",
      "Epoch 5/6 - Loss: 1.8805, Accuracy: 41.18%\n",
      "Epoch 6/6 - Loss: 1.4856, Accuracy: 37.25%\n",
      "Saving local updates for client1...\n",
      "\n",
      "Running local updates for client2...\n",
      "Epoch 1/6 - Loss: 2.0883, Accuracy: 0.00%\n",
      "Epoch 2/6 - Loss: 2.0795, Accuracy: 0.00%\n",
      "Epoch 3/6 - Loss: 2.0561, Accuracy: 10.78%\n",
      "Epoch 4/6 - Loss: 2.0026, Accuracy: 34.31%\n",
      "Epoch 5/6 - Loss: 1.8320, Accuracy: 38.24%\n",
      "Epoch 6/6 - Loss: 1.4913, Accuracy: 41.18%\n",
      "Saving local updates for client2...\n",
      "\n",
      "Running local updates for client3...\n",
      "Epoch 1/6 - Loss: 2.0867, Accuracy: 0.00%\n",
      "Epoch 2/6 - Loss: 2.0764, Accuracy: 1.96%\n",
      "Epoch 3/6 - Loss: 2.0553, Accuracy: 7.84%\n",
      "Epoch 4/6 - Loss: 2.0156, Accuracy: 28.43%\n",
      "Epoch 5/6 - Loss: 1.8894, Accuracy: 37.25%\n",
      "Epoch 6/6 - Loss: 1.4001, Accuracy: 45.10%\n",
      "Saving local updates for client3...\n",
      "\n",
      "Running local updates for client4...\n",
      "Epoch 1/6 - Loss: 2.0922, Accuracy: 0.00%\n",
      "Epoch 2/6 - Loss: 2.0806, Accuracy: 0.00%\n",
      "Epoch 3/6 - Loss: 2.0621, Accuracy: 7.84%\n",
      "Epoch 4/6 - Loss: 2.0143, Accuracy: 24.51%\n",
      "Epoch 5/6 - Loss: 1.8871, Accuracy: 35.29%\n",
      "Epoch 6/6 - Loss: 1.5855, Accuracy: 41.18%\n",
      "Saving local updates for client4...\n",
      "\n",
      "Running local updates for client5...\n",
      "Epoch 1/6 - Loss: 2.0878, Accuracy: 0.00%\n",
      "Epoch 2/6 - Loss: 2.0724, Accuracy: 2.94%\n",
      "Epoch 3/6 - Loss: 2.0543, Accuracy: 7.84%\n",
      "Epoch 4/6 - Loss: 2.0071, Accuracy: 35.29%\n",
      "Epoch 5/6 - Loss: 1.8300, Accuracy: 45.10%\n",
      "Epoch 6/6 - Loss: 1.4974, Accuracy: 29.41%\n",
      "Saving local updates for client5...\n",
      "\n",
      "Performing Server Update...\n",
      "Done...\n",
      "Assigning current server state to the global model...\n",
      "Evaluating Test Set Performance...\n",
      "Test Loss: 1.3709, Test Accuracy: 41.11%\n",
      "Done...\n",
      "\n",
      "============================================================\n",
      "------ Server Round 1 ------\n",
      "============================================================\n",
      "\n",
      "Running local updates for client1...\n",
      "Epoch 1/6 - Loss: 1.1959, Accuracy: 45.10%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Cell 8: 연합학습 실행\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m model, serverStateDict, lossDict, testLoss, accuracyDict, testAccuracy = \u001b[43mfederated_averaging\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mSERVER_ROUNDS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLOCAL_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLOCAL_BATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLOCAL_LEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclientIDs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainImageDict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainLabelDict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtestImages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestLabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\iot\\IoT_FL_AM\\utils\\cnn\\federated_averaging.py:119\u001b[39m, in \u001b[36mfederated_averaging\u001b[39m\u001b[34m(model, SERVER_ROUNDS, LOCAL_EPOCHS, LOCAL_BATCH_SIZE, LOCAL_LEARNING_RATE, clientIDs, imageDict, labelDict, testImages, testLabels, num_classes, device)\u001b[39m\n\u001b[32m    116\u001b[39m optimizer.zero_grad()\n\u001b[32m    118\u001b[39m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m outputs = \u001b[43mclientModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_images\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m loss = criterion(outputs, batch_labels)\n\u001b[32m    122\u001b[39m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\iot\\IoT_FL_AM\\venv311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\iot\\IoT_FL_AM\\venv311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\iot\\IoT_FL_AM\\utils\\cnn\\classifier.py:74\u001b[39m, in \u001b[36mCNNClassifier.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;66;03m# 다섯 번째 Conv 블록\u001b[39;00m\n\u001b[32m     73\u001b[39m x = F.relu(\u001b[38;5;28mself\u001b[39m.conv5_1(x))\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m x = F.relu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv5_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     75\u001b[39m x = \u001b[38;5;28mself\u001b[39m.pool5(x)\n\u001b[32m     77\u001b[39m \u001b[38;5;66;03m# Global Average Pooling\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\iot\\IoT_FL_AM\\venv311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\iot\\IoT_FL_AM\\venv311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\iot\\IoT_FL_AM\\venv311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\iot\\IoT_FL_AM\\venv311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:543\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    533\u001b[39m         F.pad(\n\u001b[32m    534\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    541\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    542\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Cell 8: 연합학습 실행\n",
    "model, serverStateDict, lossDict, testLoss, accuracyDict, testAccuracy = federated_averaging(\n",
    "    model,\n",
    "    SERVER_ROUNDS, LOCAL_EPOCHS, LOCAL_BATCH_SIZE, LOCAL_LEARNING_RATE,\n",
    "    clientIDs, trainImageDict, trainLabelDict,\n",
    "    testImages, testLabels,\n",
    "    num_classes,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d456d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: 학습 곡선 시각화\n",
    "plot_training_curves(lossDict, accuracyDict, testLoss, testAccuracy, clientIDs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76bdb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: 모델 저장\n",
    "import os\n",
    "from utils.cnn.classifier import save_cnn_model\n",
    "\n",
    "os.makedirs('saved_models', exist_ok=True)\n",
    "\n",
    "test_set_name = ''.join([c.replace('client', '') for c in testClients])\n",
    "if len(test_set_name) == 1:\n",
    "    test_set_name = '0' + test_set_name\n",
    "\n",
    "lr_str = f\"{LOCAL_LEARNING_RATE:.0e}\".replace('-', '').replace('+', '').replace('.0', '')\n",
    "base_filename = f'saved_models/CNN_FL_{SERVER_ROUNDS}_{LOCAL_EPOCHS}_{LOCAL_BATCH_SIZE}_{lr_str}_HoldoutPart{test_set_name}.pth'\n",
    "\n",
    "model_filename = base_filename\n",
    "counter = 1\n",
    "while os.path.exists(model_filename):\n",
    "    base_name, ext = os.path.splitext(base_filename)\n",
    "    model_filename = f'{base_name}_{counter}{ext}'\n",
    "    counter += 1\n",
    "\n",
    "save_cnn_model(model, model_filename, num_classes=num_classes)\n",
    "print(f'\\n최종 테스트 성능:')\n",
    "print(f'  Loss: {testLoss[-1]:.4f}')\n",
    "print(f'  Accuracy: {testAccuracy[-1]:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c919f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: 예측 결과 시각화\n",
    "visualize_predictions(\n",
    "    model, testImageDict, testLabelDict, testClients,\n",
    "    clientIdentifierDict, data_dir, num_samples=6, device=device\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
