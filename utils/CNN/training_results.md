# 결함 유형 분류 모델 학습 결과

## 📊 학습 개요

- **Device**: CPU
- **데이터**: 9,665개 (학습: 7,732개, 검증: 1,933개)
- **클래스**: 7개 (Normal, Super Elevation, Recoater Streaking, Fail, Laser capture timing error, Reocater Streaking, Recoater capture timing error)
- **에포크**: 20
- **배치 크기**: 32

## 🎯 주요 성능

- **최고 검증 정확도**: **92.91%** (Epoch 18)
- **최종 검증 정확도**: 92.34%
- **최종 학습 정확도**: 91.46%
- **과적합**: 없음 (검증 정확도 ≥ 학습 정확도)

## 📈 에포크별 성능

| Epoch | Train Loss | Train Acc | Val Loss | Val Acc | 최고 |
|-------|------------|-----------|----------|---------|------|
| 1 | 0.9416 | 65.26% | 0.5649 | 81.01% | ✓ |
| 5 | 0.4071 | 86.06% | 0.4147 | 86.45% | ✓ |
| 10 | 0.3118 | 89.33% | 0.2862 | 89.71% | |
| 15 | 0.2540 | 90.93% | 0.2553 | 92.24% | ✓ |
| **18** | **0.2441** | **91.46%** | **0.2295** | **92.91%** | **✓** |
| 20 | 0.2366 | 91.46% | 0.2466 | 92.34% | |

## 📊 학습 분석

### 긍정적
- ✅ 안정적인 수렴 (손실 지속 감소)
- ✅ 과적합 없음
- ✅ 빠른 초기 학습 (Epoch 1-5)

### 주의사항
- ⚠️ Epoch 7에서 검증 손실 일시 증가 후 회복
- ⚠️ Epoch 18 이후 약간의 성능 하락 (Early Stopping 고려)
- ⚠️ CPU 사용으로 학습 시간 길음 (에포크당 7-9분)

## 💡 개선 방안

1. **GPU 사용**: 학습 시간 대폭 단축
2. **Early Stopping**: Epoch 18에서 조기 종료
3. **클래스 가중치**: 소수 클래스 성능 개선
4. **데이터 증강**: 소수 클래스 강화

## 📝 결론

**92.91%의 검증 정확도**를 달성하여 우수한 성능을 보였습니다. 안정적인 학습 곡선과 과적합 없는 일반화 성능이 확인되었습니다.

**최고 모델**: `checkpoints/defect_type_classifier_best.pth` (Epoch 18)

---

## 📊 GPU 학습 결과 (2025-11-20)

### 학습 개요

- **Device**: CUDA (GPU)
- **학습 시작**: 2025-11-20 02:18:21
- **학습 종료**: 2025-11-20 04:36:19
- **총 학습 시간**: 2시간 17분 56초 (8,276.22초)
- **에포크당 평균 시간**: 27.59초
- **데이터**: 9,665개 (학습: 7,732개, 검증: 1,933개)
- **클래스**: 7개 (Normal, Super Elevation, Recoater Streaking, Fail, Laser capture timing error, Reocater Streaking, Recoater capture timing error)
- **에포크**: 300
- **배치 크기**: 64

### 하이퍼파라미터

- **학습률**: 0.001
- **Weight Decay**: 0.0001
- **이미지 크기**: 128×128
- **데이터 증강**: 활성화
- **학습률 스케줄러**: StepLR (step_size=10, gamma=0.5)
- **조기 종료 기준**: 98.0% (도달하지 못함)

### 최적화 설정

- **Mixed Precision Training (AMP)**: 활성화
- **CUDA 스트림**: 활성화
- **num_workers**: 8
- **prefetch_factor**: 8

### 주요 성능

- **최고 검증 정확도**: **93.43%** (Epoch 247)
- **최종 검증 정확도**: 93.07% (Epoch 300)
- **최종 학습 정확도**: 91.68%
- **최종 검증 손실**: 0.2449
- **최종 학습 손실**: 0.2255
- **과적합**: 없음 (검증 정확도 > 학습 정확도)

### 주요 개선 시점

| Epoch | 검증 정확도 | 비고 |
|-------|------------|------|
| 6 | 68.13% | 큰 향상 |
| 9 | 83.14% | 80% 돌파 |
| 13 | 84.95% | 85% 근접 |
| 14 | 86.45% | 85% 돌파 |
| 24 | 89.14% | 89% 돌파 |
| 30 | 90.64% | 90% 돌파 |
| 37 | 91.36% | 91% 돌파 |
| 40 | 91.67% | |
| 46 | 91.93% | |
| 49 | 92.08% | 92% 돌파 |
| 51 | 92.14% | |
| 54 | 92.40% | |
| 56 | 92.55% | |
| 60 | 92.86% | |
| 65 | 92.91% | |
| 73 | 92.96% | |
| 77 | 93.27% | 93% 돌파 |
| 106 | 93.33% | |
| 126 | 93.38% | |
| **247** | **93.43%** | **최고 성능** |

### 학습 진행 추이

#### 초기 단계 (Epoch 1-10)
- 검증 정확도: 45.47% → 81.95%
- 빠른 초기 학습 진행
- Epoch 6에서 큰 향상 (68.13%)

#### 중기 단계 (Epoch 11-50)
- 검증 정확도: 82.62% → 92.14%
- 안정적인 성능 향상
- Epoch 30에서 90% 돌파

#### 후기 단계 (Epoch 51-100)
- 검증 정확도: 92.40% → 93.33%
- 점진적 개선
- Epoch 77에서 93% 돌파

#### 최종 단계 (Epoch 101-300)
- 검증 정확도: 93.27% → 93.43% (최고)
- 성능 정체 구간
- Epoch 247에서 최고 성능 달성 후 소폭 하락

### 학습 분석

#### 긍정적 측면
- ✅ **안정적인 학습**: 검증 손실이 0.24~0.25 수준으로 안정적
- ✅ **과적합 없음**: 검증 정확도(93.07%)가 학습 정확도(91.68%)보다 높음
- ✅ **지속적 개선**: 초기 45%에서 최종 93%까지 향상
- ✅ **GPU 활용**: CUDA 사용으로 학습 속도 향상 (에포크당 27.59초)
- ✅ **Mixed Precision**: AMP 사용으로 메모리 효율성 및 속도 향상

#### 개선 필요 사항
- ⚠️ **조기 종료 미도달**: 목표 98%에 도달하지 못함 (93.43%에서 정체)
- ⚠️ **성능 정체**: Epoch 100 이후 93%대에서 정체
- ⚠️ **학습률 조정**: 후반부 학습률 감소로 추가 개선 여지

### CPU vs GPU 비교

| 항목 | CPU (이전) | GPU (현재) | 개선 |
|------|-----------|-----------|------|
| **최고 검증 정확도** | 92.91% | 93.43% | +0.52%p |
| **최종 검증 정확도** | 92.34% | 93.07% | +0.73%p |
| **에포크 수** | 20 | 300 | +280 |
| **배치 크기** | 32 | 64 | 2배 |
| **에포크당 시간** | 7-9분 | 27.59초 | **약 15배 빠름** |
| **총 학습 시간** | 약 2.5시간 | 2시간 18분 | 비슷 (에포크 수 차이) |
| **최적화** | 기본 | AMP + CUDA Stream | 고급 |

### 권장 개선 방안

1. **학습률 스케줄러 개선**
   - CosineAnnealingLR 또는 ReduceLROnPlateau 사용
   - 후반부 학습률 조정으로 성능 향상 가능

2. **모델 구조 개선**
   - 더 깊은 네트워크 또는 ResNet, EfficientNet 등 사용
   - Transfer Learning 적용

3. **데이터 증강 강화**
   - 더 다양한 증강 기법 적용 (CutMix, MixUp 등)
   - 소수 클래스에 대한 증강 강화

4. **앙상블 학습**
   - 여러 모델 조합으로 성능 향상
   - 목표 98% 달성 가능성 증가

5. **하이퍼파라미터 튜닝**
   - 배치 크기, 학습률, weight decay 등 최적화
   - Grid Search 또는 Bayesian Optimization 활용

6. **클래스 가중치 조정**
   - 소수 클래스 성능 개선
   - Focal Loss 등 불균형 데이터 대응

### 결론

**93.43%의 최고 검증 정확도**를 달성하여 이전 CPU 버전(92.91%)보다 **0.52%p 향상**되었습니다. GPU 사용으로 학습 속도가 크게 향상되었으며, 안정적인 학습 곡선과 과적합 없는 일반화 성능이 확인되었습니다.

하지만 목표 정확도 98%에는 도달하지 못했으며, Epoch 100 이후 성능이 정체되는 현상이 관찰되었습니다. 추가적인 모델 구조 개선 및 하이퍼파라미터 최적화를 통해 목표 성능에 근접할 수 있을 것으로 예상됩니다.

**최고 모델**: `checkpoints/defect_type_classifier_best.pth` (Epoch 247, Val Acc: 93.43%)

---